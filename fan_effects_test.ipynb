{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thaopham03/evaluating_fan_effects_in_large_language_models/blob/main/fan_effects_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf2JBLkJO-dd",
        "outputId": "5d7f8c5a-b680-4f66-c4ec-9f599c6fd09f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/JesseTNRoberts/minicons_modded\n",
            "  Cloning https://github.com/JesseTNRoberts/minicons_modded to /tmp/pip-req-build-r4bpoq81\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/JesseTNRoberts/minicons_modded /tmp/pip-req-build-r4bpoq81\n",
            "  Resolved https://github.com/JesseTNRoberts/minicons_modded to commit 2aa9e4e05fb5c2af99f10dc75cc1e2968b5eaceb\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/JesseTNRoberts/PopulationLM\n",
            "  Cloning https://github.com/JesseTNRoberts/PopulationLM to /tmp/pip-req-build-nucqf1o4\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/JesseTNRoberts/PopulationLM /tmp/pip-req-build-nucqf1o4\n",
            "  Resolved https://github.com/JesseTNRoberts/PopulationLM to commit e7adde2326e644d7c94d2c7086d9561f8aefb977\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.5.40)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# install external libraries:\n",
        "\n",
        "!pip install git+https://github.com/JesseTNRoberts/minicons_modded\n",
        "!pip install git+https://github.com/JesseTNRoberts/PopulationLM\n",
        "!pip install accelerate transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1IZ1GyRN1F0",
        "outputId": "e3e53873-cb0a-41d0-876e-4ed8a1c562fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaP4VXaMPFXJ",
        "outputId": "aff9876e-a175-4f28-9067-58511ce37a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import argparse\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "import accelerate\n",
        "\n",
        "import transformers\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from transformers import AutoModelForMaskedLM, AutoModelForCausalLM\n",
        "from transformers import AutoModelForMaskedLM, AutoModelForCausalLM\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from minicons import scorer\n",
        "import PopulationLM as pop\n",
        "\n",
        "# from google.colab import userdata\n",
        "# token = userdata.get('HF_TOKEN')\n",
        "\n",
        "import gc\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "committee_size = 50\n",
        "add_dropout_layers = True\n",
        "\n",
        "\n",
        "inpath = '/content/drive/MyDrive/IGOG-Alignment-Exp/Experiments/fan-effects-exp/generated_birds_prompts.csv'\n",
        "\n",
        "cachepath = '/content/drive/MyDrive/IGOG-Alignment-Exp/cache/'\n",
        "\n",
        "\n",
        "# make results dir: ../IGOG-Alignment-Exp/Experiments/fan-effects-exp/results/ration/model_name.csv\n",
        "components = inpath.split(\"/\")\n",
        "data_dir = \"/\".join(components[0:-1])\n",
        "dataset_name = components[-1].split(\".\")[0]\n",
        "results_dir = f\"{data_dir}/results/rational\"\n",
        "\n",
        "dataset = []\n",
        "with open(inpath, \"r\") as f:\n",
        "    reader = csv.DictReader(f)\n",
        "    column_names = reader.fieldnames\n",
        "    for row in reader:\n",
        "        dataset.append(list(row.values()))\n",
        "\n",
        "batch_size = 54\n",
        "device = 'cuda'\n",
        "stimuli_loader = DataLoader(dataset, batch_size = batch_size, num_workers=0)\n",
        "\n",
        "models = [\n",
        "    ['bert-base-uncased',  'mlm'],\n",
        "    # ['bert-large-uncased', 'mlm'],\n",
        "    # ['distilbert-base-uncased', 'mlm'],\n",
        "    # ['roberta-base', 'mlm'],\n",
        "    # ['roberta-large', 'mlm'],\n",
        "    # [\"google/electra-large-generator\", 'mlm'],\n",
        "    # ['FacebookAI/xlm-roberta-large', 'mlm'],\n",
        "    # ['distilgpt2', 'incremental'],\n",
        "    # ['gpt2-medium',  'incremental'],\n",
        "    # ['gpt2',  'incremental'],\n",
        "    # ['openai-gpt',  'incremental'],\n",
        "    # [\"daryl149/llama-2-7b-hf\", 'incremental'],\n",
        "    # [\"mistralai/Mistral-7B-v0.1\",  'incremental'],\n",
        "    # [\"google/gemma-7b\",  'incremental'],\n",
        "    # ['PKU-Alignment/alpaca-7b-reproduced', 'incremental'],\n",
        "    # ['openlm-research/open_llama_13b',  'incremental'],\n",
        "    # ['upstage/SOLAR-10.7B-v1.0',  'incremental'],\n",
        "    # ['microsoft/phi-2',  'incremental'],\n",
        "    # ['meta-llama/Llama-2-13b-hf',  'incremental'],\n",
        "]\n",
        "\n",
        "use_population = True\n",
        "local_only = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paWcs1u1PF4a"
      },
      "outputs": [],
      "source": [
        "for model in models:\n",
        "    model_name = model[0].replace(\"/\", \"_\")\n",
        "    if os.path.exists(results_dir + f\"/{model_name}.csv\"):\n",
        "        continue\n",
        "\n",
        "    print(model[0])\n",
        "\n",
        "    try:\n",
        "        if model[1] == \"mlm\":\n",
        "            # try:\n",
        "                transformer = scorer.MaskedLMScorer(model[0], device, token=None, cache_dir=cachepath,\n",
        "                                                    local_files_only=local_only, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "            # except ValueError as e:\n",
        "            #     if \"device_map\" in str(e):\n",
        "            #         transformer = scorer.MaskedLMScorer(model[0], device, token=None, cache_dir=cachepath,\n",
        "            #                                             local_files_only=local_only, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
        "            #     else:\n",
        "            #         raise e\n",
        "        elif model[1] == \"incremental\":\n",
        "            # try:\n",
        "                transformer = scorer.IncrementalLMScorer(model[0], device, token=None, cache_dir=cachepath,\n",
        "                                                    local_files_only=local_only, low_cpu_mem_usage=True, torch_dtype=torch.float16, device_map=\"auto\")\n",
        "        #     except ValueError as e:\n",
        "        #         if \"device_map\" in str(e):\n",
        "        #             transformer = scorer.IncrementalLMScorer(model[0], device, token=None, cache_dir=cachepath,\n",
        "        #                                                     local_files_only=local_only, low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
        "        #         else:\n",
        "        #             raise e\n",
        "        # else:\n",
        "        #     raise ValueError(\"Invalid model type\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model {model[0]}: {e}\")\n",
        "        continue\n",
        "\n",
        "    num_params = [sum(p.numel() for p in transformer.model.parameters())] * len(dataset)\n",
        "\n",
        "    for val in [0.1]:\n",
        "        if use_population:\n",
        "            pop.DropoutUtils.convert_dropouts(transformer.model)\n",
        "            pop.DropoutUtils.activate_mc_dropout(transformer.model, activate=True, random=val)\n",
        "\n",
        "        results = []\n",
        "        control_results = []\n",
        "        conclusion_only = []\n",
        "\n",
        "        call_me = lambda prefixes, queries: transformer.conditional_score(prefixes, queries, reduction=lambda x: (x.sum(0).item(), x.mean(0).item(), x.tolist()))\n",
        "\n",
        "        for batch in stimuli_loader:\n",
        "            premise = list(batch[0])\n",
        "            conclusion = list(batch[1])\n",
        "\n",
        "            if use_population:\n",
        "                population = pop.generate_dropout_population(transformer.model, lambda: call_me(premise, conclusion), committee_size=committee_size)\n",
        "\n",
        "            print(len(premise), len(conclusion))\n",
        "\n",
        "            if use_population:\n",
        "                outs = [item for item in tqdm(pop.call_function_with_population(transformer.model, population, lambda: call_me(premise, conclusion)),\n",
        "                                              total=committee_size)]\n",
        "            else:\n",
        "                outs = [call_me(premise, conclusion)]\n",
        "\n",
        "            transposed_outs = [[row[i] for row in outs] for i in range(len(outs[0]))]\n",
        "            priming_scores = [score for score in transposed_outs]\n",
        "            results.extend(priming_scores)\n",
        "\n",
        "        data_out = list(zip(*dataset))\n",
        "        new_col_names = column_names\n",
        "\n",
        "        data_out.append(results)\n",
        "        new_col_names += [\"score (sum, mean, [list)\"]\n",
        "\n",
        "        data_out.append(num_params)\n",
        "        data_out.append([model_name] * len(results))\n",
        "        new_col_names += [\"params\", \"model\"]\n",
        "\n",
        "        with open(results_dir + f\"/{model_name}\"+\".csv\", \"w\") as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(new_col_names)\n",
        "            writer.writerows(list(zip(*data_out)))\n",
        "            f.flush()\n",
        "\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}